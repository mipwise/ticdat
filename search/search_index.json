{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ticdat Go here for project status and installation instructions. Go here for documentation. ticdat is a Python package that provides lightweight, ORM style functionality around either a dict-of-dicts or pandas.DataFrame representation of tables. It is well suited for defining and validating the input data for complex solve engines (i.e. optimization and scheduling-type problems). ticdat functionality is organized around two classes - TicDatFactory and PanDatFactory . Both classes define a simple database style schema on construction. Data integrity rules can then be added in the form of foreign key relationships, data field types (to include numerical ranges and allowed strings) and row predicates (functions that check if a given row violates a particular data condition). The factory classes can then be used to construct TicDat / PanDat objects that contain tables consistent with the defined schema. By design, ticdat , allows these data objects to violate the data integrity rules while providing convenient bulk query functions to determine where those violations occur. TicDat objects (created by a TicDatFactory ) contain tables in a dict-of-dict format. The outer dictionary maps primary key values to data rows. The inner dictionaries are data rows indexed by field names (similar to csv.DictReader/csv.DictWriter ). Tables that do not have primary keys are rendered as a list of data row dictionaries. PanDat objects (created by PanDatFactory ) render tables as pandas.DataFrame objects. The columns in each DataFrame will contain all of the primary key and data fields that were defined in the PanDatFactory schema. The PanDatFactory code can be thought of as implementing a shim library that organizes DataFrame objects into a predefined schema, and facilitates rich integrity checks based on schema defined rules. The ticdat example library is focused on two patterns for building optimization engines - using TicDatFactory in conjunction with gurobipy and using PanDatFactory in conjunction with amplpy . That said, ticdat can also be used with libraries like pyomo , pulp , docplex and xpress . It also has functionality to support the OPL and LINGO modeling languages, although the AMPL support is far more mature. ticdat is also useful for machine-learning applications. In this case, one typically uses PanDatFactory to provide ORM-like functionality on top of pandas , as well as to simplify the munging of time stamp data and text columns that contain exclusively numbers. The ticdat library is distributed under the BSD2 open source license.","title":"Home"},{"location":"#ticdat","text":"Go here for project status and installation instructions. Go here for documentation. ticdat is a Python package that provides lightweight, ORM style functionality around either a dict-of-dicts or pandas.DataFrame representation of tables. It is well suited for defining and validating the input data for complex solve engines (i.e. optimization and scheduling-type problems). ticdat functionality is organized around two classes - TicDatFactory and PanDatFactory . Both classes define a simple database style schema on construction. Data integrity rules can then be added in the form of foreign key relationships, data field types (to include numerical ranges and allowed strings) and row predicates (functions that check if a given row violates a particular data condition). The factory classes can then be used to construct TicDat / PanDat objects that contain tables consistent with the defined schema. By design, ticdat , allows these data objects to violate the data integrity rules while providing convenient bulk query functions to determine where those violations occur. TicDat objects (created by a TicDatFactory ) contain tables in a dict-of-dict format. The outer dictionary maps primary key values to data rows. The inner dictionaries are data rows indexed by field names (similar to csv.DictReader/csv.DictWriter ). Tables that do not have primary keys are rendered as a list of data row dictionaries. PanDat objects (created by PanDatFactory ) render tables as pandas.DataFrame objects. The columns in each DataFrame will contain all of the primary key and data fields that were defined in the PanDatFactory schema. The PanDatFactory code can be thought of as implementing a shim library that organizes DataFrame objects into a predefined schema, and facilitates rich integrity checks based on schema defined rules. The ticdat example library is focused on two patterns for building optimization engines - using TicDatFactory in conjunction with gurobipy and using PanDatFactory in conjunction with amplpy . That said, ticdat can also be used with libraries like pyomo , pulp , docplex and xpress . It also has functionality to support the OPL and LINGO modeling languages, although the AMPL support is far more mature. ticdat is also useful for machine-learning applications. In this case, one typically uses PanDatFactory to provide ORM-like functionality on top of pandas , as well as to simplify the munging of time stamp data and text columns that contain exclusively numbers. The ticdat library is distributed under the BSD2 open source license.","title":"ticdat"},{"location":"csvtd/","text":"CsvTicFactory CsvTicFactory(self, tic_dat_factory) Primary class for reading/writing csv files with TicDat objects. Your system will need the csv package if you want to use this class. Don't create this object explicitly. A CsvTicFactory will automatically be associated with the csv attribute of the parent create_tic_dat CsvTicFactory.create_tic_dat(self, dir_path, dialect='excel', headers_present=True, freeze_it=False, encoding=None) Create a TicDat object from the csv files in a directory :param dir_path: the directory containing the .csv files. :param dialect: the csv dialect. Consult csv documentation for details. :param headers_present: Boolean. Does the first row of data contain the column headers? :param encoding: see docstring for the Python.open function :param freeze_it: boolean. should the returned object be frozen? :return: a TicDat object populated by the matching files. caveats: Missing files resolve to an empty table, but missing fields on matching files throw an Exception. By default, data field values (but not primary key values) will be coerced into floats if possible. This includes coercing \"inf\" and \"-inf\" into +/- float(\"inf\") The rules over which fields are/are-not coerced can be controlled via hints from the default values or data types. Default values and data types can also be used to control whether or not the empty string should be coerced into None. The infinity_io_flag rules are applied subsequent to this coercion. Note - pandas doesn't really do a fantastic job handling date types either, since it coerces all columns to be the same type. If that's the behavior you want you can use PanDatFactory. JSON is just a better file format than csv for a variety of reasons, to include typing of data. find_duplicates CsvTicFactory.find_duplicates(self, dir_path, dialect='excel', headers_present=True, encoding=None) Find the row counts for duplicated rows. :param dir_path: the directory containing .csv files. :param dialect: the csv dialect. Consult csv documentation for details. :param headers_present: Boolean. Does the first row of data contain the column headers? :param encoding: see docstring for the Python.open function :return: A dictionary whose keys are the table names for the primary key tables. Each value of the return dictionary is itself a dictionary. The inner dictionary is keyed by the primary key values encountered in the table, and the value is the count of records in the Excel sheet with this primary key. Row counts smaller than 2 are pruned off, as they aren't duplicates caveats: Missing files resolve to an empty table, but missing fields (data or primary key) on matching files throw an Exception. write_directory CsvTicFactory.write_directory(self, tic_dat, dir_path, allow_overwrite=False, dialect='excel', write_header=True, case_space_table_names=False) write the ticDat data to a collection of csv files :param tic_dat: the data object :param dir_path: the directory in which to write the csv files :param allow_overwrite: boolean - are we allowed to overwrite existing files? :param dialect: the csv dialect. Consult csv documentation for details. :param write_header: Boolean. Should the header information be written as the first row? :param case_space_table_names: boolean - make best guesses how to add spaces and upper case characters to table names :return:","title":"CsvTicFactory"},{"location":"jsontd/","text":"make_json_dict make_json_dict(tdf, tic_dat, verbose=False, use_infinity_io_flag_if_provided=False) JsonTicFactory JsonTicFactory(self, tic_dat_factory) Primary class for reading/writing json files with TicDat objects. You need the json package to be installed to use it. create_tic_dat JsonTicFactory.create_tic_dat(self, json_file_path, freeze_it=False, from_pandas=False) Create a TicDat object from a json file :param json_file_path: A json file path. It should encode a dictionary with table names as keys. Could also be an actual JSON string :param freeze_it: boolean. should the returned object be frozen? :param from_pandas: boolean. If truthy, then use pandas json readers. See PanDatFactory json readers for more details. This argument is historical, as a json format that matches the PanDatFactory.json format will be detected automatically, and thus client code is generally safe ignoring this argument completely. :return: a TicDat object populated by the matching tables. caveats: Table names matches are case insensitive and also underscore-space insensitive. Tables that don't find a match are interpreted as an empty table. Dictionary keys that don't match any table are ignored. find_duplicates JsonTicFactory.find_duplicates(self, json_file_path, from_pandas=False) Find the row counts for duplicated rows. :param json_file_path: A json file path. It should encode a dictionary with table names as keys. :param from_pandas: boolean. If truthy, then use pandas json readers. See PanDatFactory json readers for more details. :return: A dictionary whose keys are table names for the primary-ed key tables. Each value of the return dictionary is itself a dictionary. The inner dictionary is keyed by the primary key values encountered in the table, and the value is the count of records in the json entry with this primary key. Row counts smaller than 2 are pruned off, as they aren't duplicates write_file JsonTicFactory.write_file(self, tic_dat, json_file_path, allow_overwrite=False, verbose=False, to_pandas=False) write the ticDat data to a json file (or json string) :param tic_dat: the data object to write (typically a TicDat) :param json_file_path: The file path of the json file to create. If empty string, then return a JSON string. :param allow_overwrite: boolean - are we allowed to overwrite an existing file? :param verbose: boolean. Verbose mode writes the data rows as dicts keyed by field name. Otherwise, they are lists. :param to_pandas: boolean. if truthy, then use the PanDatFactory method of writing to json. :return:","title":"JsonTicFactory"},{"location":"mdb/","text":"MdbTicFactory MdbTicFactory(self, tic_dat_factory) Primary class for reading/writing Access/MDB files with TicDat objects. Don't create this object explicitly. A MdbTicDatFactory will automatically be associated with the mdb attribute of the parent TicDatFactory. Your system will need the pypyodbc package if you want to actually do something with it. can_write_new_file :return: True if this environment can write to a new mdb database files, False otherwise create_tic_dat MdbTicFactory.create_tic_dat(self, mdb_file_path, freeze_it=False) Create a TicDat object from an Access MDB file :param mdb_file_path: An Access db with a consistent schema. :param freeze_it: boolean. should the returned object be frozen? :return: a TicDat object populated by the matching tables. caveats : Tables that don't find a match are interpreted as an empty table. Missing fields on matching tables throw an exception. Also, see infinity_io_flag find_duplicates MdbTicFactory.find_duplicates(self, mdb_file_path) Find the row counts for duplicated rows. :param mdb_file_path: An Access db with a consistent schema. :return: A dictionary whose keys are table names for the primary-ed key tables. Each value of the return dictionary is itself a dictionary. The inner dictionary is keyed by the primary key values encountered in the table, and the value is the count of records in the mdb table with this primary key. Row counts smaller than 2 are pruned off, as they aren't duplicates write_schema MdbTicFactory.write_schema(self, mdb_file_path, **field_types) Populate an Access file with a database schema :param mdb_file_path: The file path of the mdb database to create :param field_types: Named arguments are table names. Argument values are mapping of field name to field type. Allowable field types are text, double and int If missing, primary key fields are text, and data fields are double :return: write_file MdbTicFactory.write_file(self, tic_dat, mdb_file_path, allow_overwrite=False) write the ticDat data to an SQLite database file :param tic_dat: the data object to write :param mdb_file_path: the file path of the SQLite database to populate :param allow_overwrite: boolean - are we allowed to overwrite pre-existing data :return: caveats : See infinity_io_flag NB - thrown Exceptions of the form \"Data type mismatch in criteria expression\" generally result either from Access's inability to store different data types in the same field, or from a mismatch between the data object and the default field types ticdat uses when creating an Access schema. For the latter, feel free to call the write_schema function on the data file first with explicitly identified field types.","title":"MdbTicFactory"},{"location":"model/","text":"Model Model(self, model_type='gurobi', model_name='model') Mixed Integer Programming modeling object. Provides pass-through functionality to one of the three main commercial MIP Python APIs (CPLEX, Gurobi, and XPRESS). add_var Model.add_var(self, lb=0, ub=inf, type='continuous', name='') Add a variable to the model. :param lb: The lower bound of the variable. :param ub: The upper bound of the variable. :param type: either 'binary' or 'continuous' :param name: The name of the variable. (Ignored if falsey). :return: The variable object associated with the model_type engine API add_constraint Model.add_constraint(self, constraint, name='') Add a constraint to the model. :param constraint: A constraint created by via linear or quadratic combination of variables and numbers. Be sure to use Model.sum for summing over iterables. :param name: The name of the constraint. Ignored if falsey. :return: The constraint object associated with the model_type engine API set_objective Model.set_objective(self, expression, sense='minimize') Set the objective for the model. :param expression: A linear or quadratic combination of variables and numbers. Be sure to use Model.sum for summing over iterables. :param sense: Either 'minimize' or 'maximize' :return: None set_parameters Model.set_parameters(self, **kwargs) Set one or more parameters in the core model :param kwargs: A mapping of parameter keyword to parameter value. The keywords need to be on the list of known parameters, as follows. MIP_Gap : set the MIP optimization tolerance :return: None optimize Model.optimize(self, *args, **kwargs) Optimize the model. :param args: Optional engine-specific arguments to pass to the model_type API :param kwargs: Optional engine-specific arguments to pass to the model_type API :return: Truthy if the model solves successfully, Falsey otherwise. For a cplex model, a sucessful optimization returns the cplex.Model.solve result get_solution_value Model.get_solution_value(self, var) Get the value for a variable in the solution. Only call after a successful optimize() call. :param var: The variable returned from a pervious call to add_var() :return: The value of this variable in the optimal solution.","title":"Model"},{"location":"opl/","text":"opl_run opl_run(mod_file, input_tdf, input_dat, soln_tdf, infinity=999999, oplrun_path=None) solve an optimization problem using an OPL .mod file :param mod_file: An OPL .mod file. :param input_tdf: A TicDatFactory defining the input schema :param input_dat: A TicDat object consistent with input_tdf :param soln_tdf: A TicDatFactory defining the solution schema :param infinity: A number used to represent infinity in OPL :return: a TicDat object consistent with soln_tdf, or None if no solution found create_opl_text create_opl_text(tdf, tic_dat, infinity=999999) Generate a OPL .dat string from a TicDat object :param tdf: A TicDatFactory defining the schema :param tic_dat: A TicDat object consistent with tdf :param infinity: A number used to represent infinity in OPL :return: A string consistent with the OPL .dat format create_opl_mod_text create_opl_mod_text(tdf) Generate a OPL .mod string from a TicDat object for diagnostic purposes :param tdf: A TicDatFactory defining the input schema :return: A string consistent with the OPL .mod input format create_opl_mod_output_text create_opl_mod_output_text(tdf) Generate a OPL .mod string from a TicDat object for diagnostic purposes :param tdf: A TicDatFactory defining the input schema :return: A string consistent with the OPL .mod output format read_opl_text read_opl_text(tdf, text, commaseperator=True) Read an OPL .dat string :param tdf: A TicDatFactory defining the schema :param text: A string consistent with the OPL .dat format :return: A TicDat object consistent with tdf","title":"OPL"},{"location":"pandatfactory/","text":"PanDatFactory PanDatFactory(self, **init_fields) Defines a schema for a collection of pandas.DataFrame objects. This class is constructed with a schema. It can be used to generate PanDat objects, to write PanDat objects to different file types, or to perform bulk query operations to diagnose common data integrity failures. Analytical code that uses PanDat objects can be used, without change, on different data sources, thus facilitating the \"separate model from data\" design goal. A PanDat object is itself a collection of DataFrames that conform to a predefined schema. :param init_fields: a mapping of tables to primary key fields and data fields. Each field listing consists of two sub lists ... first primary keys fields, then data fields. ex: PanDatFactory (categories = [[\"name\"],[\"Min Nutrition\", \"Max Nutrition\"]], foods = [[\"Name\"],[\"Cost\"]] nutritionQuantities = [[\"Food\", \"Category\"],[\"Qty\"]]) Use '*' instead of a pair of lists for generic tables ex: PanDatFactory (typical_table = [[\"Primary Key Field\"],[\"Data Field\"]], generic_table = '*') add_parameter PanDatFactory.add_parameter(self, name, default_value, number_allowed=True, inclusive_min=True, inclusive_max=False, min=0, max=inf, must_be_int=False, strings_allowed=(), nullable=False, datetime=False, enforce_type_rules=True) Add (or reset) a parameters option. Requires that a parameters table with one primary key field and one data field already be present. The legal parameters options will be enforced as part of find_data_row_failures Note that if you are using this function, then you would typically read from the parameters table indirectly, by using the dictionary returned by create_full_parameters_dict. :param name: name of the parameter to add or reset :param default_value: default value for the parameter (used for create_full_parameters_dict) :param number_allowed: boolean does this parameter allow numbers? :param inclusive_min: if number allowed, is the min inclusive? :param inclusive_max: if number allowed, is the max inclusive? :param min: if number allowed, the minimum value :param max: if number allowed, the maximum value :param must_be_int: boolean : if number allowed, must the number be integral? :param strings_allowed: if a collection - then a list of the strings allowed. The empty collection prohibits strings. If a \"*\", then any string is accepted. :param nullable: boolean : can this parameter be set to null (aka None) :param datetime: If truthy, then number_allowed through strings_allowed are ignored. Should the data either be a datetime.datetime object or a string that can be parsed into a datetime.datetime object? Note that the various readers will try to coerce strings into datetime.datetime objects on read for parameters with datetime data types. pandas.Timestamp is itself a datetime.datetime, and the bias will be to create such an object. :param enforce_type_rules: boolean: ignore all of number_allowed through datetime, and only enforce the parameter names and default values :return: add_data_row_predicate PanDatFactory.add_data_row_predicate(self, table, predicate, predicate_name=None, predicate_kwargs_maker=None, predicate_failure_response='Boolean') The purpose of calling add_data_row_predicate is to prepare for a future call to find_data_row_failures. See https://bit.ly/3e9pdCP for more details on these two functions. Adds a data row predicate for a table. Row predicates can be used to check for sophisticated data integrity problems of the sort that can't be easily handled with a data type rule. For example, a min_supply column can be verified to be no larger than a max_supply column. !!! NB!!!! pandas will render None as nan. Don't check for None in your predicate functions, use pandas.isnull instead !!!!!!!!!! :param table: table in the schema :param predicate: A one argument function that accepts a table row as an argument and returns Truthy if the row is valid and Falsey otherwise. (See below, there are other arguments that can refine how predicate works). The row argument passed to predicate will be a dict that maps field name to data value for all fields (both primary key and data field) in the table. Note - if None is passed as a predicate, then any previously added predicate matching (table, predicate_name) will be removed. . :param predicate_name: The name of the predicate. If omitted, the smallest non-colliding number will be used. :param predicate_kwargs_maker: A function used to support predicate if predicate accepts more than just the row argument. This function accepts a single dat argument and is called exactly once per find_data_row_failures call. If predicate_kwargs_maker returns a dict, then this dict is unpacked for each call to predicate. An error (or a bulk row failure) results if predicate_kwargs_maker fails to return a dict. :param predicate_failure_response: Either \"Boolean\" or \"Error Message\". If the latter then predicate indicates a clean row by returning True (the one and only literal True in Python) and a dirty row by returning a non-empty string (which is an error message). See find_data_row_failures for details on handling exceptions thrown by predicate or predicate_kwargs_maker. :return: add_foreign_key PanDatFactory.add_foreign_key(self, native_table, foreign_table, mappings) Adds a foreign key relationship to the schema. Adding a foreign key doesn't block the entry of child records that fail to find a parent match. It does make it easy to recognize such records (with find_foreign_key_failures()) and to remove such records (with remove_foreign_key_failures()) :param native_table: (aka child table). The table with fields that must match some other table. :param foreign_table: (aka parent table). The table providing the matching entries. :param mappings: For simple foreign keys, a [native_field, foreign_field] pair. For compound foreign keys an iterable of [native_field, foreign_field] pairs. :return: clear_data_type PanDatFactory.clear_data_type(self, table, field) clears the data type for a field. By default, fields don't have types. Adding a data type doesn't block data of the wrong type from being entered. Data types are useful for recognizing errant data entries. If no data type is specified (the default) then no errant data will be recognized. :param table: table in the schema :param field: one of table's fields. :return: clear_foreign_keys PanDatFactory.clear_foreign_keys(self, native_table=None) create a PanDatFactory :param native_table: optional. The table whose foreign keys should be cleared. If omitted, all foreign keys are cleared. clone PanDatFactory.clone(self, table_restrictions=None, clone_factory=None) clones the PanDatFactory :param table_restrictions : if None, then argument is ignored. Otherwise, a container listing the tables to keep in the clone. Tables outside table_restrictions are removed from the clone. :param clone_factory : optional. Defaults to PanDatFactory. Can also be TicDatFactory. Can also be a function, in which case it should behave similarly to create_from_full_schema. If clone_factory=TicDatFactory, the row predicates that use predicate_kwargs_maker won't be copied over. :return: a clone of the PanDatFactory. Returned object will be based on clone_factory, if provided. Note - If you want to remove tables via a clone, then call like this pdf_new = pdf.clone(table_restrictions=set(pdf.all_tables).difference(tables_to_remove)) Other schema editing operations are available with clone_add_a_table, clone_add_a_column, clone_remove_a_column and clone_rename_a_column. copy_from_ampl_variables PanDatFactory.copy_from_ampl_variables(self, ampl_variables) copies the solution results from ampl_variables into a new PanDat object :param ampl_variables: a dict mapping from (table_name, field_name) -> amplpy.variable.Variable (amplpy.variable.Variable is the type object returned by AMPL.getVariable) table_name should refer to a table in the schema that has primary key fields. field_name can refer to a data field for table_name, or it can be falsey. If the latter, then AMPL variables that pass the filter (see below) will simply populate the primary key of the table_name. Note that by default, only non-zero data is copied over. If you want to override this filter, then instead of mapping to amplpy.variable.Variable you should map to a (amplpy.variable.Variable, filter) where filter accepts a data value and returns a boolean. :return: a deep copy of the ampl_variables into a PanDat object copy_pan_dat PanDatFactory.copy_pan_dat(self, pan_dat) copies the tic_dat object into a new tic_dat object performs a deep copy :param pan_dat: a pandat object :return: a deep copy of the pan_dat argument copy_to_ampl PanDatFactory.copy_to_ampl(self, pan_dat, field_renamings=None, excluded_tables=None) copies the pan_dat object into a new pan_dat object populated with amplpy.DataFrame objects performs a deep copy :param pan_dat: a PanDat object :param field_renamings: dict or None. If fields are to be renamed in the copy, then a mapping from (table_name, field_name) -> new_field_name If a data field is to be omitted, then new_field can be falsey table_name cannot refer to an excluded table. (see below) field_name doesn't have to refer to a field to an element of self.data_fields[t], but it doesn't have to refer to a column in the pan_dat.table_name DataFrame :param excluded_tables: If truthy, a list of tables to be excluded from the copy. Tables without primary key fields are always excluded. :return: a deep copy of the tic_dat argument into amplpy.DataFrames create_from_full_schema PanDatFactory.create_from_full_schema(full_schema) create a PanDatFactory complete with default values, data types, and foreign keys :param full_schema: a dictionary consistent with the data returned by a call to schema() with include_ancillary_info = True :return: a PanDatFactory reflecting the tables, fields, default values, data types, and foreign keys consistent with the full_schema argument create_full_parameters_dict PanDatFactory.create_full_parameters_dict(self, dat) create a fully populated dictionary of all the parameters :param dat: a PanDat object that has a parameters table :return: a dictionary that maps parameter option to actual dat.parameters value. if the specific option isn't part of dat.parameters, then the default value is used. Note that for datetime parameters, the default will be coerced into a datetime object, if possible. find_data_row_failures PanDatFactory.find_data_row_failures(self, pan_dat, as_table=True, exception_handling='__debug__', max_failures=inf) Finds the data row failures for a ticdat object :param pan_dat: a pandat object :param as_table: boolean - if truthy then the values of the return dictionary will be the predicate failure rows themselves. Otherwise will return the boolean Series that indicates which rows have predicate failures. :param exception_handling: One of \"Handled as Failure\", \"Unhandled\" or \" debug \" \"Handled as Failure\": Any exception generated by calling a row predicate function will indicate a data failure for that row. (Similarly, predicate_kwargs_maker exceptions create an entry in the returned failure dictionary). \"Unhandled\": Exceptions resulting from calling a row predicate (or a predicate_kwargs_maker) will not be handled by data_row_failures. \" debug \": Since \"Handled as Failure\" makes more sense for production runs and \"Unhandled\" makes more sense for debugging, this option will use the latter if debug is True and the former otherwise. See -o and debug in Python documentation for more details. :param max_failures: number. An upper limit on the number of failures to find. Will short circuit and return ASAP with a partial failure enumeration when this number is reached. :return: A dictionary constructed as follows: The keys are namedtuples with members \"table\", \"predicate_name\". The values are DataFrames that contain the subset of rows that exhibit data failures for this specific table, predicate pair (or the Series that identifies these rows). If the predicate_failure_response for the predicate is \"Error Message\" (instead of \"Boolean\") and as_table is truthy, then an \"Error Message\" column will be added to the appropriate DataFrame in the returned dict. If a predicate_kwargs_maker is provided and it fails (either by failing to return a dictionary or by throwing a handled exception) then appropriate value of the dictionary will be a namedtuple with members \"primary_key\" and \"error message\". The former will be populated with '*' (indicating all the rows) and the latter will be a string describing the failure. find_data_type_failures PanDatFactory.find_data_type_failures(self, pan_dat, as_table=True, max_failures=inf) Finds the data type failures for a pandat object :param pan_dat: pandat object :param as_table: boolean - if truthy then the values of the return dictionary will be the data type failure rows themselves. Otherwise will return the boolean Series that indicates which rows have data type failures. :param max_failures: number. An upper limit on the number of failures to find. Will short circuit and return ASAP with a partial failure enumeration when this number is reached. :return: A dictionary constructed as follow: The keys are namedtuples with members \"table\", \"field\". Each (table,field) pair has data values that are inconsistent with its data type. (table, field) pairs with no data type at all are never part of the returned dictionary. The values are DataFrames that contain the subset of rows that exhibit data failures for this specific table, field pair (or the boolean Series that identifies these rows). Note that for primary key fields (but not data fields) with no explicit data type, a temporary filter that excludes only Null will be applied. If you want primary key fields to allow Null, you must explicitly opt-in by calling set_data_type appropriately. See issue https://github.com/ticdat/ticdat/issues/46 for more info. find_duplicates PanDatFactory.find_duplicates(self, pan_dat, keep='first', as_table=True) Find the duplicated rows based on the primary key fields. :param pan_dat: pandat object :param keep: 'first': Treat all duplicated rows as duplicates except for the first occurrence. 'last': Treat all duplicated rows as duplicates except for the last occurrence. False: Treat all duplicated rows as duplicates :param as_table: as_table boolean : if truthy then the values of the return dictionary will be the duplicated rows themselves. Otherwise will return the boolean Series that indicates which rows are duplicated rows. :return: A dictionary whose keys are the table names and whose values are duplicated rows (or the Series that identifies these rows) find_foreign_key_failures PanDatFactory.find_foreign_key_failures(self, pan_dat, verbosity='High', as_table=True, max_failures=inf) Finds the foreign key failures for a pandat object :param pan_dat: pandat object :param verbosity: either \"High\" or \"Low\" :param as_table: as_table boolean : if truthy then the values of the return dictionary will be the failed rows themselves. Otherwise will return the a boolean list that indicates which rows have failures. (For technical reasons, not returning a boolean Series like the other find functions) :param max_failures: number. An upper limit on the number of failures to find. Will short circuit and return ASAP with a partial failure enumeration when this number is reached. :return: A dictionary constructed as follows: The keys are namedtuples with members \"native_table\", \"foreign_table\", \"mapping\", \"cardinality\". The key data matches the arguments to add_foreign_key that constructed the foreign key (with \"cardinality\" being deduced from the overall schema). The values are DataFrames that contain the subset of native table rows that fail to find the foreign table matching defined by the associated returned key (or the list that identifies these rows). For verbosity = 'Low' a simpler return object is created that doesn't use namedtuples and omits the foreign key cardinality. good_pan_dat_object PanDatFactory.good_pan_dat_object(self, data_obj, bad_message_handler=<function PanDatFactory.<lambda> at 0x119535f28>) determines if an object is a valid PanDat object for this schema :param data_obj: the object to verify :param bad_message_handler: a call back function to receive description of any failure message :return: True if the dataObj can be recognized as a PanDat data object. False otherwise. remove_foreign_key_failures PanDatFactory.remove_foreign_key_failures(self, pan_dat) Removes foreign key failures (i.e. child records with no parent table record) :param pan_dat: pandat object (will be side-effected) :return: pan_dat, with the foreign key failures removed Note that all foreign key removals are cascading. When a child removal results in new foreign key failures, those failures are removed as well. schema PanDatFactory.schema(self, include_ancillary_info=False) Return a dictionary that summarizes the schema. :param include_ancillary_info: if True, include all the foreign key, default, and data type information as well. Otherwise, just return table-fields dictionary :return: a dictionary with table name mapping to a list of lists defining primary key fields and data fields If include_ancillary_info, this table-fields dictionary is just one entry in a more comprehensive dictionary. set_ampl_data PanDatFactory.set_ampl_data(self, ampl_dat, ampl, table_to_set_name=None) performs bulk setData on the AMPL-esque first argument. :param ampl_dat: an AmplTicDat object created by calling copy_to_ampl :param ampl: an amplpy.AMPL object :param table_to_set_name: a mapping of table_name to ampl set name :return: set_data_type PanDatFactory.set_data_type(self, table, field, number_allowed=True, inclusive_min=True, inclusive_max=False, min=0, max=inf, must_be_int=False, strings_allowed=(), nullable=False, datetime=False) sets the data type for a field. By default, fields don't have types. Adding a data type doesn't block data of the wrong type from being entered. Data types are useful for recognizing errant data entries with find_data_type_failures(). Errant data entries can be replaced with replace_data_type_failures(). :param table: a table in the schema :param field: a data field for this table :param number_allowed: boolean does this field allow numbers? :param inclusive_min: boolean : if number allowed, is the min inclusive? :param inclusive_max: boolean : if number allowed, is the max inclusive? :param min: if number allowed, the minimum value :param max: if number allowed, the maximum value :param must_be_int: boolean : if number allowed, must the number be integral? :param strings_allowed: if a collection - then a list of the strings allowed. The empty collection prohibits strings. If a \"*\", then any string is accepted. :param nullable : boolean : can this value contain null (aka None aka nan (since pandas treats null as nan)) :param datetime: If truthy, then number_allowed through strings_allowed are ignored. Should the data either be a datetime.datetime object or a string that can be parsed into a datetime.datetime object? Note that the various readers will try to coerce strings into datetime.datetime objects on read for fields with datetime data types. pandas.Timestamp is itself a datetime.datetime, and the bias will be to create such an object. :return: set_default_value PanDatFactory.set_default_value(self, table, field, default_value) sets the default value for a specific field :param table: a table in the schema :param field: a field in the table :param default_value: the default value to apply Note - the data fields of a schema will have the default default of zero. The primary key fields will have no default at all (NOT None, but rather, no default). replace_data_type_failures will only perform replacements on fields for which there is a default, unless there is some explicit override provided. (see replace_data_type_failures for details). This is deliberate, since a bulk replacement in a primary key field is likely to create a duplication failure. :return: set_default_values PanDatFactory.set_default_values(self, **table_defaults) sets the default values for the fields :param table_defaults: A dictionary of named arguments. Each argument name (i.e. each key) should be a table name Each value should itself be a dictionary mapping data field names to default values Ex: pdf.set_default_values(categories = {\"minNutrition\":0, \"maxNutrition\":float(\"inf\")}, foods = {\"cost\":0}, nutritionQuantities = {\"qty\":0}) :return: set_infinity_io_flag PanDatFactory.set_infinity_io_flag(self, value) Set the infinity_io_flag for the PanDatFactory. 'N/A' (the default) is recognized as a flag to disable infinity I/O buffering. If numeric, when writing data to the file system (or a database), float(\"inf\") will be replaced by the infinity_io_flag and float(\"-inf\") will be replaced by -infinity_io_flag, prior to writing. Similarly, the read data will replace any number >= the infinity_io_flag with float(\"inf\") and any number smaller than float(\"-inf\") with -infinity_io_flag. If None, then +/- infinity will be replaced by None prior to writing. Similarly, subsequent to reading, None will be replaced either by float(\"inf\") or float(\"-inf\"), depending on field data types. Note that None flagging will only perform replacements on fields whose data types allow infinity and not None. For all cases, these replacements will be done on a temporary copy of the data that is created prior to writing. Also note that none of the these replacements will be done on the parameters table. The assumption is the parameters table will be serialized to a string/string database table. Infinity can thus be represented by \"inf\"/\"-inf\" in such serializations. :param value: a valid infinity_io_flag :return:","title":"PanDatFactory"},{"location":"pandatio/","text":"CsvPanFactory CsvPanFactory(self, pan_dat_factory) Primary class for reading/writing csv files with PanDat objects. Don't create this object explicitly. A CsvPanFactory will automatically be associated with the csv attribute of the parent PanDatFactory. create_pan_dat CsvPanFactory.create_pan_dat(self, dir_path, fill_missing_fields=False, **kwargs) Create a PanDat object from a directory of csv files. :param db_file_path: the directory containing the .csv files. :param fill_missing_fields: boolean. If truthy, missing fields will be filled in with their default value. Otherwise, missing fields throw an Exception. :param kwargs: additional named arguments to pass to pandas.read_csv :return: a PanDat object populated by the matching tables. caveats: Missing tables always throw an Exception. Table names are matched with case-space insensitivity, but spaces are respected for field names. (ticdat supports whitespace in field names but not table names). Note that if you save a DataFrame to csv and then recover it, the type of data might change. For example df = pd.DataFrame({\"a\":[\"100\", \"200\", \"300\"]}) df.to_csv(\"something.csv\") df2 = pd.read_csv(\"something.csv\") results in a numeric column in df2. To address this, you need to either use set_data_type for your PanDatFactory, or specify \"dtype\" in kwargs. (The former is obviously better). This problem is even worse with df = pd.DataFrame({\"a\":[\"0100\", \"1200\", \"2300\"]}) write_directory CsvPanFactory.write_directory(self, pan_dat, dir_path, case_space_table_names=False, index=False, **kwargs) write the PanDat data to a collection of csv files :param pan_dat: the PanDat object to write :param dir_path: the directory in which to write the csv files Set to falsey if using con argument. :param case_space_table_names: boolean - make best guesses how to add spaces and upper case characters to table names :param index: boolean - whether or not to write the index. :param kwargs: additional named arguments to pass to pandas.to_csv :return: caveats: The row names (index) isn't written (unless kwargs indicates it should be). JsonPanFactory JsonPanFactory(self, pan_dat_factory) Primary class for reading/writing json data with PanDat objects. Don't create this object explicitly. A JsonPanFactory will automatically be associated with the json attribute of the parent PanDatFactory. create_pan_dat JsonPanFactory.create_pan_dat(self, path_or_buf, fill_missing_fields=False, orient='split', **kwargs) Create a PanDat object from a JSON file or string :param path_or_buf: a valid JSON string or file-like :param fill_missing_fields: boolean. If truthy, missing fields will be filled in with their default value. Otherwise, missing fields throw an Exception. Doesn't work with list-of-lists format. :param orient: Indication of expected JSON string format. See pandas.read_json for more details. :param kwargs: additional named arguments to pass to pandas.read_json :return: a PanDat object populated by the matching tables. caveats: Missing tables always resolve to an empty table. Table names are matched with case-space insensitivity, but spaces are respected for field names. (ticdat supports whitespace in field names but not table names). Note that if you save a DataFrame to json and then recover it, the type of data might change. Specifically, text that looks numeric might be recovered as a number, to include the loss of leading zeros. To address this, you need to either use set_data_type for your PanDatFactory, or specify \"dtype\" in kwargs. (The former is obviously better). write_file JsonPanFactory.write_file(self, pan_dat, json_file_path) Write the PanDat data to a json file (or json string). Writes each table as a list-of-lists. See write_file_pd for other formats. :param pan_dat: the PanDat object to write :param json_file_path: the json file into which the data is to be written. If falsey, will return a JSON string :return: A JSON string if json_file_path is falsey, otherwise None write_file_pd JsonPanFactory.write_file_pd(self, pan_dat, json_file_path, case_space_table_names=False, orient='split', index=False, indent=2, sort_keys=False, **kwargs) write the PanDat data to a json file (or json string). Use this routine to write json text that is consistent with what pandas.to_json. The list-of-lists format is created with write_file. In older ticdat releases, write_file implemented the functionaltiy now provided with write_file_pd. :param pan_dat: the PanDat object to write :param json_file_path: the json file into which the data is to be written. If falsey, will return a JSON string :param case_space_table_names: boolean - make best guesses how to add spaces and upper case characters to table names :param orient: Passed through to pandas.to_json. Default of \"split\", combined with index=False, writes a smaller json file. :param index: boolean - whether or not to write the index. :param indent: 2. See json.dumps :param sort_keys: See json.dumps :param kwargs: additional named arguments to pass to pandas.to_json :return: NB - pandas seems stubbornly unable to inject Infinity into json, but it can read Infinity from json. We work around this with a GUID created flagging string when encountering float(\"inf\"), float(-\"inf\"). SqlPanFactory SqlPanFactory(self, pan_dat_factory) Primary class for reading/writing SQLite files with PanDat objects. Don't create this object explicitly. A SqlPanFactory will automatically be associated with the sql attribute of the parent PanDatFactory. create_pan_dat SqlPanFactory.create_pan_dat(self, db_file_path, con=None, fill_missing_fields=False) Create a PanDat object from a SQLite database file :param db_file_path: A SQLite DB File. Set to falsey if using con argument :param con: A connection object that can be passed to pandas read_sql. Set to falsey if using db_file_path argument. :param fill_missing_fields: boolean. If truthy, missing fields will be filled in with their default value. Otherwise, missing fields throw an Exception. :return: a PanDat object populated by the matching tables. caveats: Missing tables always resolve to an empty table, but missing fields on matching tables throw an exception (unless fill_missing_fields is truthy). Table names are matched with case-space insensitivity, but spaces are respected for field names. (ticdat supports whitespace in field names but not table names). write_file SqlPanFactory.write_file(self, pan_dat, db_file_path, con=None, if_exists='replace', case_space_table_names=False) write the PanDat data to an excel file :param pan_dat: the PanDat object to write :param db_file_path: The file path of the SQLite file to create. Set to falsey if using con argument. :param con: A connection object that can be passed to pandas to_sql. Set to falsey if using db_file_path argument :param if_exists: \u2018fail\u2019, \u2018replace\u2019 or \u2018append\u2019. How to behave if the table already exists :param case_space_table_names: boolean - make best guesses how to add spaces and upper case characters to table names :return: caveats: The row names (index) isn't written. The default pandas schema generation is used, and thus foreign key relationships aren't written. XlsPanFactory XlsPanFactory(self, pan_dat_factory) Primary class for reading/writing Excel files with panDat objects. Don't create this object explicitly. A XlsPanFactory will automatically be associated with the xls attribute of the parent PanDatFactory. create_pan_dat XlsPanFactory.create_pan_dat(self, xls_file_path, fill_missing_fields=False) Create a PanDat object from an Excel file :param xls_file_path: An Excel file containing sheets whose names match the table names in the schema. :param fill_missing_fields: boolean. If truthy, missing fields will be filled in with their default value. Otherwise, missing fields throw an Exception. :return: a PanDat object populated by the matching sheets. caveats: Missing sheets resolve to an empty table, but missing fields on matching sheets throw an Exception (unless fill_missing_fields is truthy). Table names are matched to sheets with with case-space insensitivity, but spaces and case are respected for field names. (ticdat supports whitespace in field names but not table names). Note that if you save a DataFrame to excel and then recover it, the type of data might change. For example df = pd.DataFrame({\"a\":[\"100\", \"200\", \"300\"]}) df.to_excel(\"something.xlsx\") df2 = pd.read_excel(\"something.xlsx\") results in a numeric column in df2. To address this, you need to use set_data_type for your PanDatFactory. This problem is even worse with df = pd.DataFrame({\"a\":[\"0100\", \"1200\", \"2300\"]}) write_file XlsPanFactory.write_file(self, pan_dat, file_path, case_space_sheet_names=False) write the panDat data to an excel file :param pan_dat: the PanDat object to write :param file_path: The file path of the excel file to create :param case_space_sheet_names: boolean - make best guesses how to add spaces and upper case characters to sheet names :return: caveats: The row names (index) isn't written.","title":"XlsPanFactory"},{"location":"pgtd/","text":"PostgresPanFactory PostgresPanFactory(self, pan_dat_factory) Primary class for reading/writing PostGres databases with PanDat objects. Don't create this object explicitly. A PostgresPanFactory will automatically be associated with the pgsql attribute of the parent PanDatFactory. Will need to have pandas installed to do anything. postgres doesn't support brackets, and putting spaces in postgres field names is frowned upon. https://bit.ly/2xWLZL3. You are encouraged to continue to use field names like \"Min Nutrition\" in your ticdat Python code, and the pgtd code here will match such fields up with postgres field names like min_nutrition when reading/writing from a postgres DB. (Non alphamnumeric characters in general, and not just spaces, are replaced with underscores for generating PGSQL field names). create_pan_dat PostgresPanFactory.create_pan_dat(self, engine, schema, active_fld='') Create a PanDat object from a PostGres connection :param engine: A sqlalchemy connection to the PostGres database :param schema : The name of the schema to read from :param active_fld: if provided, a string for a boolean filter field. Must be compliant w PG naming conventions, which are different from ticdat field naming conventions. Typically developer can ignore this argument, designed for expert support. :return: a PanDat object populated by the matching tables. Missing tables issue a warning and resolve to empty. write_data PostgresPanFactory.write_data(self, pan_dat, engine, schema, pre_existing_rows=None, active_fld='', progress=None) write the PanDat data to a postgres database :param pan_dat: a PanDat object :param engine: A sqlalchemy connection to the PostGres database :param schema: The postgres schema to write to (call self.write_schema explicitly as needed) :param pre_existing_rows: if provided, a dict mapping table name to either \"delete\" or \"append\" default behavior is \"delete\" :param active_fld: if provided, a string for a boolean filter field which will be populated with True. Must be compliant w PG naming conventions, which are different from ticdat field naming conventions. Typically developer can ignore this argument, designed for expert support. :param progress: if provided, a ticdat.Progress object that is called every time a table is uploaded :return: PostgresTicFactory PostgresTicFactory(self, tic_dat_factory) Primary class for reading/writing PostGres databases with TicDat objects. You need the sqlalchemy package to be installed to use it. Don't create this object explicitly. A PostgresTicFactory will automatically be associated with the pgsql attribute of the parent TicDatFactory. postgres doesn't support brackets, and putting spaces in postgres field names is frowned upon. https://bit.ly/2xWLZL3. You are encouraged to continue to use field names like \"Min Nutrition\" in your ticdat Python code, and the pgtd code here will match such fields up with postgres field names like min_nutrition when reading/writing from a postgres DB. (Non alphamnumeric characters in general, and not just spaces, are replaced with underscores for generating PGSQL field names) create_tic_dat PostgresTicFactory.create_tic_dat(self, engine, schema, freeze_it=False, active_fld='') Create a TicDat object from a PostGres connection :param engine: A sqlalchemy connection to the PostGres database :param schema : The name of the schema to read from :param freeze_it: boolean. should the returned object be frozen? :param active_fld: if provided, a string for a boolean filter field. Must be compliant w PG naming conventions, which are different from ticdat field naming conventions. Typically developer can ignore this argument, designed for expert support. :return: a TicDat object populated by the matching tables. Missing tables issue a warning and resolve to empty. find_duplicates PostgresTicFactory.find_duplicates(self, engine, schema, active_fld='') Find the row counts for duplicated rows. :param engine: A sqlalchemy Engine object that can connect to our postgres instance :param schema: Name of the schema within the engine's database to use :param active_fld: if provided, a string for a boolean filter field. Must be compliant w PG naming conventions, which are different from ticdat field naming conventions. Typically developer can ignore this argument, designed for expert support. :return: A dictionary whose keys are table names for the primary-ed key tables. Each value of the return dictionary is itself a dictionary. The inner dictionary is keyed by the primary key values encountered in the table, and the value is the count of records in the postgres table with this primary key. Row counts smaller than 2 are pruned off, as they aren't duplicates write_data PostgresTicFactory.write_data(self, tic_dat, engine, schema, dsn=None, pre_existing_rows=None, active_fld='') write the ticDat data to a PostGres database :param tic_dat: the data object to write :param engine: a sqlalchemy database engine with drivertype postgres :param schema: the postgres schema to write to (call self.write_schema explicitly as needed) :param dsn: optional - if truthy, a dict that can be unpacked as arguments to psycopg2.connect. Will speed up bulk writing compared to engine.execute If truthy and not a dict, then will be passed directly to psycopg2.connect as the sole argument. :param pre_existing_rows: if provided, a dict mapping table name to either \"delete\" or \"append\" default behavior is \"delete\" :param active_fld: if provided, a string for a boolean filter field which will be populated with True. Must be compliant w PG naming conventions, which are different from ticdat field naming conventions. Typically developer can ignore this argument, designed for expert support. :return:","title":"PostgresTicFactory"},{"location":"sqlitetd/","text":"SQLiteTicFactory SQLiteTicFactory(self, tic_dat_factory) Primary class for reading/writing SQLite files with TicDat objects. Don't create this object explicitly. A SQLiteTicFactory will automatically be associated with the sql attribute of the parent TicDatFactory. You need the sqlite3 package to be installed to use it. create_tic_dat SQLiteTicFactory.create_tic_dat(self, db_file_path, freeze_it=False) Create a TicDat object from a SQLite database file :param db_file_path: A SQLite db with a consistent schema. :param freeze_it: boolean. should the returned object be frozen? :return: a TicDat object populated by the matching tables. caveats : \"inf\" and \"-inf\" (case insensitive) are read as floats, unless the infinity_io_flag is being applied. \"true\"/\"false\" (case insensitive) are read as booleans booleans. Tables that don't find a match are interpreted as an empty table. Missing fields on matching tables throw an exception. create_tic_dat_from_sql SQLiteTicFactory.create_tic_dat_from_sql(self, sql_file_path, includes_schema=False, freeze_it=False) Create a TicDat object from an SQLite sql text file :param sql_file_path: A text file containing SQLite compatible SQL statements delimited by ; :param includes_schema: boolean - does the sql_file_path contain schema generating SQL? :param freeze_it: boolean. should the returned object be frozen? :return: a TicDat object populated by the db created from the SQL. See create_tic_dat for caveats. find_duplicates SQLiteTicFactory.find_duplicates(self, db_file_path) Find the row counts for duplicated rows. :param db_file_path: A SQLite db with a consistent schema. :return: A dictionary whose keys are table names for the primary-ed key tables. Each value of the return dictionary is itself a dictionary. The inner dictionary is keyed by the primary key values encountered in the table, and the value is the count of records in the SQLite table with this primary key. Row counts smaller than 2 are pruned off, as they aren't duplicates write_db_schema SQLiteTicFactory.write_db_schema(self, db_file_path) :param db_file_path: the file path of the SQLite database to create :return: write_db_data SQLiteTicFactory.write_db_data(self, tic_dat, db_file_path, allow_overwrite=False) write the ticDat data to an SQLite database file :param tic_dat: the data object to write :param db_file_path: the file path of the SQLite database to populate :param allow_overwrite: boolean - are we allowed to overwrite pre-existing data :return: caveats : True, False are written as \"True\", \"False\". Also see infinity_io_flag doc write_sql_file SQLiteTicFactory.write_sql_file(self, tic_dat, sql_file_path, include_schema=False, allow_overwrite=False) write the sql for the ticDat data to a text file :param tic_dat: the data object to write :param sql_file_path: the path of the text file to hold the sql statements for the data :param include_schema: boolean - should we write the schema sql first? :param allow_overwrite: boolean - are we allowed to overwrite pre-existing file :return: caveats : float(\"inf\"), float(\"-inf\") are written as \"inf\", \"-inf\" (unless infinity_io_flag is being applied). True/False are written as \"True\", \"False\"","title":"SQLiteTicFactory"},{"location":"ticdatfactory/","text":"TicDatFactory TicDatFactory(self, **init_fields) Primary class for ticdat library. This class is constructed with a schema. It can be used to generate TicDat objects, to write TicDat objects to different file types, or to perform bulk query operations to diagnose common data integrity failures. Analytical code that uses TicDat objects can be used, without change, on different data sources, thus facilitating the \"separate model from data\" design goal. :param init_fields: a mapping of tables to primary key fields and data fields. Each field listing consists of two sub lists ... first primary keys fields, than data fields. ex: TicDatFactory (categories = [[\"name\"],[\"minNutrition\", \"maxNutrition\"]], foods = [[\"name\"],[\"cost\"]] nutritionQuantities = [[\"food\", \"category\"],[\"qty\"]]) Use '*' instead of a pair of lists for generic tables, which will render as pandas.DataFrame objects. ex: TicDatFactory (typical_table = [[\"primary key field\"],[\"data field\"]], generic_table = '*') add_parameter TicDatFactory.add_parameter(self, name, default_value, number_allowed=True, inclusive_min=True, inclusive_max=False, min=0, max=inf, must_be_int=False, strings_allowed=(), nullable=False, datetime=False, enforce_type_rules=True) Add (or reset) a parameters option. Requires that a parameters table with one primary key field and one data field already be present. The legal parameters options will be enforced as part of find_data_row_failures Note that if you are using this function, then you would typically read from the parameters table indirectly, by using the dictionary returned by create_full_parameters_dict. :param name: name of the parameter to add or reset :param default_value: default value for the parameter (used for create_full_parameters_dict) :param number_allowed: boolean does this parameter allow numbers? :param inclusive_min: if number allowed, is the min inclusive? :param inclusive_max: if number allowed, is the max inclusive? :param min: if number allowed, the minimum value :param max: if number allowed, the maximum value :param must_be_int: boolean : if number allowed, must the number be integral? :param strings_allowed: if a collection - then a list of the strings allowed. The empty collection prohibits strings. If a \"*\", then any string is accepted. :param nullable: boolean : can this parameter be set to null (aka None) :param datetime: If truthy, then number_allowed through strings_allowed are ignored. Should the data either be a datetime.datetime object or a string that can be parsed into a datetime.datetime object? :param enforce_type_rules: boolean: ignore all of number_allowed through nullable, and only enforce the parameter names and default values :return: add_data_row_predicate TicDatFactory.add_data_row_predicate(self, table, predicate, predicate_name=None, predicate_kwargs_maker=None, predicate_failure_response='Boolean') The purpose of calling add_data_row_predicate is to prepare for a future call to find_data_row_failures. See https://bit.ly/3e9pdCP for more details on these two functions. Adds a data row predicate for a table. Row predicates can be used to check for sophisticated data integrity problems of the sort that can't be easily handled with a data type rule. For example, a min_supply column can be verified to be no larger than a max_supply column. :param table: table in the schema :param predicate: A one argument function that accepts a table row as an argument and returns Truthy if the row is valid and Falsey otherwise. (See below, there are other arguments that can refine how predicate works). The row argument passed to predicate will be a dict that maps field name to data value for all fields (both primary key and data field) in the table. Note - if None is passed as a predicate, then any previously added predicate matching (table, predicate_name) will be removed. :param predicate_name: The name of the predicate. If omitted, the smallest non-colliding number will be used. :param predicate_kwargs_maker: A function used to support predicate if predicate accepts more than just the row argument. This function accepts a single dat argument and is called exactly once per find_data_row_failures call. If predicate_kwargs_maker returns a dict, then this dict is unpacked for each call to predicate. An error (or a bulk row failure) results if predicate_kwargs_maker fails to return a dict. :param predicate_failure_response: Either \"Boolean\" or \"Error Message\". If the latter then predicate indicates a clean row by returning True (the one and only literal True in Python) and a dirty row by returning a non-empty string (which is an error message). See find_data_row_failures for details on handling exceptions thrown by predicate or predicate_kwargs_maker. :return: add_foreign_key TicDatFactory.add_foreign_key(self, native_table, foreign_table, mappings) Adds a foreign key relationship to the schema. Adding a foreign key doesn't block the entry of child records that fail to find a parent match. It does make it easy to recognize such records (with find_foreign_key_failures()) and to remove such records (with remove_foreign_key_failures()) :param native_table: (aka child table). The table with fields that must match some other table. :param foreign_table: (aka parent table). The table providing the matching entries. :param mappings: For simple foreign keys, a [native_field, foreign_field] pair. For compound foreign keys an iterable of [native_field, foreign_field] pairs. :return: clear_data_type TicDatFactory.clear_data_type(self, table, field) clears the data type for a field. By default, fields don't have types. Adding a data type doesn't block data of the wrong type from being entered. Data types are useful for recognizing errant data entries. If no data type is specified (the default) then no errant data will be recognized. :param table: table in the schema :param field: :return: clear_foreign_keys TicDatFactory.clear_foreign_keys(self, native_table=None) create a TicDatFactory :param native_table: optional. The table whose foreign keys should be cleared. If omitted, all foreign keys are cleared. clone TicDatFactory.clone(self, table_restrictions=None, clone_factory=None) clones the TicDatFactory :param table_restrictions : if None, then argument is ignored. Otherwise, a container listing the tables to keep in the clone. Tables outside table_restrictions are removed from the clone. :param clone_factory : optional. Defaults to TicDatFactory. Can also be PanDatFactory. Can also be a function, in which case it should behave similarly to create_from_full_schema. If clone_factory=PanDatFactory, the row predicates that use predicate_kwargs_maker won't be copied over. :return: a clone of the TicDatFactory. Returned object will based on clone_factory, if provided. Note - If you want to remove tables via a clone, then call like this tdf_new = tdf.clone(table_restrictions=set(tdf.all_tables).difference(tables_to_remove)) Other schema editing operations are available with clone_add_a_table, clone_add_a_column, clone_remove_a_column and clone_rename_a_column. copy_from_ampl_variables TicDatFactory.copy_from_ampl_variables(self, ampl_variables) copies the solution results from ampl_variables into a new ticdat object :param ampl_variables: a dict mapping from (table_name, field_name) -> amplpy.variable.Variable (amplpy.variable.Variable is the type object returned by AMPL.getVariable) table_name should refer to a table in the schema that has primary key fields. field_name can refer to a data field for table_name, or it can be falsey. If the latter, then AMPL variables that pass the filter (see below) will simply populate the primary key of the table_name. Note that by default, only non-zero data is copied over. If you want to override this filter, then instead of mapping to amplpy.variable.Variable you should map to a (amplpy.variable.Variable, filter) where filter accepts a data value and returns a boolean. :return: a deep copy of the ampl_variables into a ticdat object copy_tic_dat TicDatFactory.copy_tic_dat(self, tic_dat, freeze_it=False) copies the tic_dat object into a new tic_dat object performs a deep copy :param tic_dat: a ticdat object :param freeze_it: boolean. should the returned object be frozen? :return: a deep copy of the tic_dat argument copy_to_ampl TicDatFactory.copy_to_ampl(self, tic_dat, field_renamings=None, excluded_tables=None) copies the tic_dat object into a new tic_dat object populated with amplpy.DataFrame objects performs a deep copy :param tic_dat: a ticdat object :param field_renamings: dict or None. If fields are to be renamed in the copy, then a mapping from (table_name, field_name) -> new_field_name If a data field is to be omitted, then new_field can be falsey table_name cannot refer to an excluded table. (see below) :param excluded_tables: If truthy, a list of tables to be excluded from the copy. Tables without primary key fields are always excluded. :return: a deep copy of the tic_dat argument into amplpy.DataFrames copy_to_pandas TicDatFactory.copy_to_pandas(self, tic_dat, table_restrictions=None, drop_pk_columns=None, reset_index=False) copies the tic_dat object into a new object populated with pandas.DataFrame objects performs a deep copy :param tic_dat: a ticdat object :param table_restrictions: If truthy, a list of tables to turn into data frames. Defaults to all tables. :param drop_pk_columns: boolean or None. should the primary key columns be dropped from the data frames after they have been incorporated into the index. If None, then pk fields will be dropped only for tables with data fields :param reset_index: boolean. If true, then drop_pk_columns is ignored and the returned DataFrames have a simple integer index with both primary key and data fields as columns. :return: a deep copy of the tic_dat argument into DataFrames To get a valid pan_object object, either set drop_pk_columns to False or set reset_index to True. I.e. copy_1 = tdf.copy_to_pandas(dat, drop_pk_columns=False) copy_2 = tdf.copy_to_pandas(dat, reset_index=True) assert all(PanDatFactory(**tdf.schema()).good_pan_dat_object(_) for _ in [copy_1, copy_2]) Note that None will be converted to nan in the returned object (as is the norm for pandas.DataFrame) create_from_full_schema TicDatFactory.create_from_full_schema(full_schema) create a TicDatFactory complete with default values, data types, and foreign keys :param full_schema: a dictionary consistent with the data returned by a call to schema() with include_ancillary_info = True :return: a TicDatFactory reflecting the tables, fields, default values, data types, and foreign keys consistent with the full_schema argument create_full_parameters_dict TicDatFactory.create_full_parameters_dict(self, dat) create a fully populated dictionary of all the parameters :param dat: a TicDat object that has a parameters table :return: a dictionary that maps parameter option to actual dat.parameters value. if the specific option isn't part of dat.parameters, then the default value is used. Note that for datetime parameters, the default will be coerced into a datetime object, if possible. enable_foreign_key_links TicDatFactory.enable_foreign_key_links(self) call to enable foreign key links. For ex. a TicDat object made from a factory with foreign key enabled will pass the following assert assert (dat.foods[\"chicken\"].nutritionQuantities[\"protein\"] is dat.categories[\"protein\"].nutritionQuantities[\"chicken\"] is dat.nutritionQuantities[\"chicken\", \"protein\"]) Note that by default, TicDatFactories don't create foreign key links since doing so can slow down TicDat creation. :return: find_data_row_failures TicDatFactory.find_data_row_failures(self, tic_dat, exception_handling='__debug__', max_failures=inf) Finds the data row failures for a ticdat object :param tic_dat: ticdat object :param exception_handling: One of \"Handled as Failure\", \"Unhandled\" or \" debug \" \"Handled as Failure\": Any exception generated by calling a row predicate function will indicate a data failure for that row. (Similarly, predicate_kwargs_maker exceptions create an entry in the returned failure dictionary). \"Unhandled\": Exceptions resulting from calling a row predicate (or a predicate_kwargs_maker) will not be handled by data_row_failures. \" debug \": Since \"Handled as Failure\" makes more sense for production runs and \"Unhandled\" makes more sense for debugging, this option will use the latter if debug is True and the former otherwise. See -o and debug in Python documentation for more details. :param max_failures: number. An upper limit on the number of failures to find. Will short circuit and return ASAP with a partial failure enumeration when this number is reached. :return: A dictionary constructed as follow: The keys are namedtuples with members \"table\", \"predicate_name\". The values of the returned dictionary are tuples indicating which rows failed the predicate test. For tables with a primary key this tuple will contain the primary key value of each failed row. Otherwise, this tuple will list the positions of the failed rows. If the predicate_failure_response for the predicate is \"Error Message\" (instead of \"Boolean\") then the values of the returned dict will themselves be namedtuples with members \"primary_key\" and \"error_message\". If a predicate_kwargs_maker is provided and it fails (either by failing to return a dictionary or by throwing a handled exception) then a similar namedtuple is entered as the value, with primary_key='*' and error_message as a string. find_data_type_failures TicDatFactory.find_data_type_failures(self, tic_dat, max_failures=inf) Finds the data type failures for a ticdat object :param tic_dat: ticdat object :param max_failures: number. An upper limit on the number of failures to find. Will short circuit and return ASAP with a partial failure enumeration when this number is reached. :return: A dictionary constructed as follow: The keys are namedtuples with members \"table\", \"field\". Each (table,field) pair has data values that are inconsistent with its data type. (table, field) pairs with no data type at all are never part of the returned dictionary. The values of the returned dictionary are namedtuples with the following attributes. --> bad_values - the distinct values for the (table, field) pair that are inconsistent with the data type for (table, field). --> pks - the distinct primary key entries of the table containing the bad_values data. (will be row index for tables with no primary key) That is to say, bad_values tells you which values in field are failing the data type check, and pks tells you which table rows will have their field entry changed if you call replace_data_type_failures(). Note that for primary key fields (but not data fields) with no explicit data type, a temporary filter that excludes only Null will be applied. If you want primary key fields to allow Null, you must explicitly opt-in by calling set_data_type appropriately. See issue https://github.com/ticdat/ticdat/issues/46 for more info. find_foreign_key_failures TicDatFactory.find_foreign_key_failures(self, tic_dat, verbosity='High', max_failures=inf) Finds the foreign key failures for a ticdat object :param tic_dat: ticdat object :param max_failures: number. An upper limit on the number of failures to find. Will short circuit and return ASAP with a partial failure enumeration when this number is reached. :param verbosity: either \"High\" or \"Low\" :return: A dictionary constructed as follow (for verbosity = 'High'): The keys are namedtuples with members \"native_table\", \"foreign_table\", \"mapping\", \"cardinality\". The key data matches the arguments to add_foreign_key that constructed the foreign key (with \"cardinality\" being deduced from the overall schema). The values are namedtuples with the following members. --> native_values - the values of the native fields that failed to match --> native_pks - the primary key entries of the native table rows corresponding to the native_values. That is to say, native_values tells you which values in the native table can't find a foreign key match, and thus generate a foreign key failure. native_pks tells you which native table rows will be removed if you call remove_foreign_key_failures(). For verbosity = 'Low' a simpler return object is created that doesn't use namedtuples and omits the foreign key cardinality. freeze_me TicDatFactory.freeze_me(self, tic_dat) Freezes a ticdat object :param tic_dat: ticdat object :return: tic_dat, after it has been frozen good_tic_dat_object TicDatFactory.good_tic_dat_object(self, data_obj, bad_message_handler=<function TicDatFactory.<lambda> at 0x11be52268>, row_checking='strict') determines if an object can be can be converted to a TicDat data object. :param data_obj: the object to verify :param bad_message_handler: a call back function to receive description of any failure message :param row_checking: either \"generous\" or \"strict\". If the latter, then we expect all the rows to be dicts with the correct columns (except for things like generic tables) defaults to strict since this is the protector for the solve functions :return: True if the dataObj can be converted to a TicDat data object. False otherwise. good_tic_dat_table TicDatFactory.good_tic_dat_table(self, data_table, table_name, bad_message_handler=<function TicDatFactory.<lambda> at 0x11be52488>, row_checking='generous') determines if an object can be can be converted to a TicDat data table. :param dataObj: the object to verify :param table_name: the name of the table :param bad_message_handler: a call back function to receive description of any failure message :param row_checking: either \"generous\" or \"strict\". If the latter, then we expect all the rows to be dicts with the correct columns (except for things like generic tables) defaults to generous since this gets used a lot internally :return: True if the dataObj can be converted to a TicDat data table. False otherwise. obfusimplify TicDatFactory.obfusimplify(self, tic_dat, table_prepends={}, skip_tables=(), freeze_it=False) copies the tic_dat object into a new, obfuscated, simplified tic_dat object :param tic_dat: a ticdat object :param table_prepends: a dictionary with mapping each table to the prepend it should apply when its entries are renamed. A valid table prepend must be all caps and not end with I. Should be restricted to entity tables (single field primary that is not a foreign key child) :param skip_tables: a listing of entity tables whose single field primary key shouldn't be renamed :param freeze_it: boolean. should the returned copy be frozen? :return: A named tuple with the following components. copy : a deep copy of the tic_dat argument, with the single field primary key values renamed to simple \"short capital letters followed by numbers\" strings. renamings : a dictionary matching the new entries to their original (table, primary key value) this entry can be used to cross reference any diagnostic information gleaned from the obfusimplified copy to the original names. For example, \"P5 has no production\" can easily be recognized as \"Product KX12212 has no production\". remove_foreign_key_failures TicDatFactory.remove_foreign_key_failures(self, tic_dat, propagate=True) Removes foreign key failures (i.e. child records with no parent table record) :param tic_dat: ticdat object :param propagate boolean: remove cascading failures? (if removing the child record results in new failures, should those be removed as well?) :return: tic_dat, with the foreign key failures removed replace_data_type_failures TicDatFactory.replace_data_type_failures(self, tic_dat, replacement_values={}) Replace the data cells with data type failures with the default value for the appropriate field. :param tic_dat: a TicDat object appropriate for this schema :param replacement_values: a dictionary mapping (table, field) to replacement value. the default value will be used for (table, field) pairs not in replacement_values :return: the tic_dat object with replacements made. The tic_dat object itself will be edited in place. Replaces any of the data failures found in find_data_type_failures() with the appropriate replacement_value. Note - won't perform primary key replacements. schema TicDatFactory.schema(self, include_ancillary_info=False) :param include_ancillary_info: if True, include all the foreign key, default, and data type information as well. Otherwise, just return table-fields dictionary :return: a dictionary with table name mapping to a list of lists defining primary key fields and data fields If include_ancillary_info, this table-fields dictionary is just one entry in a more comprehensive dictionary. set_ampl_data TicDatFactory.set_ampl_data(self, tic_dat, ampl, table_to_set_name=None) performs bulk setData on the AMPL first argument. :param tic_dat: an AmplTicDat object created by calling copy_to_ampl :param ampl: an amplpy.AMPL object :param table_to_set_name: a mapping of table_name to ampl set name :return: set_data_type TicDatFactory.set_data_type(self, table, field, number_allowed=True, inclusive_min=True, inclusive_max=False, min=0, max=inf, must_be_int=False, strings_allowed=(), nullable=False, datetime=False) sets the data type for a field. By default, fields don't have types. Adding a data type doesn't block data of the wrong type from being entered. Data types are useful for recognizing errant data entries with find_data_type_failures(). Errant data entries can be replaced with replace_data_type_failures(). :param table: a table in the schema :param field: a field for this table :param number_allowed: boolean does this field allow numbers? :param inclusive_min: boolean : if number allowed, is the min inclusive? :param inclusive_max: boolean : if number allowed, is the max inclusive? :param min: if number allowed, the minimum value :param max: if number allowed, the maximum value :param must_be_int: boolean : if number allowed, must the number be integral? :param strings_allowed: if a collection - then a list of the strings allowed. The empty collection prohibits strings. If a \"*\", then any string is accepted. :param nullable : boolean : can this value contain null (aka None) :param datetime: If truthy, then number_allowed through strings_allowed are ignored. Should the data either be a datetime.datetime object or a string that can be parsed into a datetime.datetime object? Note that the various readers will try to coerce strings into datetime.datetime objects on read for fields with datetime data types. pandas.Timestamp is itself a datetime.datetime, and the bias will be to create such an object. :return: set_default_value TicDatFactory.set_default_value(self, table, field, default_value) sets the default value for a specific field :param table: a table in the schema :param field: a field in the table :param default_value: the default value to apply :return: set_default_values TicDatFactory.set_default_values(self, **tableDefaults) sets the default values for the fields :param tableDefaults: A dictionary of named arguments. Each argument name (i.e. each key) should be a table name Each value should itself be a dictionary mapping data field names to default values Ex: tdf.set_default_values(categories = {\"minNutrition\":0, \"maxNutrition\":float(\"inf\")}, foods = {\"cost\":0}, nutritionQuantities = {\"qty\":0}) :return: set_generator_tables TicDatFactory.set_generator_tables(self, g) sets which tables are to be generator tables. Generator tables are represented as generators pulled from the actual data store. This prevents them from being fulled loaded into memory. Generator tables are only appropriate for truly massive data tables with no primary key. :param g: An iterable of table name. :return: set_infinity_io_flag TicDatFactory.set_infinity_io_flag(self, value) Set the infinity_io_flag for the TicDatFactory. 'N/A' (the default) is recognized as a flag to disable infinity I/O buffering. If numeric, when writing data to the file system (or a database), float(\"inf\") will be replaced by the infinity_io_flag and float(\"-inf\") will be replaced by -infinity_io_flag, prior to writing. Similarly, the read data will replace any number >= the infinity_io_flag with float(\"inf\") and any number smaller than float(\"-inf\") with -infinity_io_flag. If None, then +/- infinity will be replaced by None prior to writing. Similarly, subsequent to reading, None will be replaced either by float(\"inf\") or float(\"-inf\"), depending on field data types. Note that None flagging will only perform replacements on fields whose data types allow infinity and not None. For all cases, these replacements will be done on a temporary copy of the data that is created prior to writing. Also note that none of the these replacements will be done on the parameters table. The assumption is the parameters table will be serialized to a string/string database table. Infinity can thus be represented by \"inf\"/\"-inf\" in such serializations. File readers will attempt to cast strings to floats on a row-by-row basis, as determined by add_parameter settings. File writers will cast parameters table entries to strings (assuming the add_parameters functionality is being used). :param value: a valid infinity_io_flag :return: freeze_me freeze_me(x) Freezes a ticdat object :param x: ticdat object :return: x, after it has been frozen","title":"TicDatFactory"},{"location":"utils/","text":"standard_main standard_main(input_schema, solution_schema, solve, case_space_table_names=False) provides standardized command line functionality for a ticdat solve engine :param input_schema: a TicDatFactory or PanDatFactory defining the input schema :param solution_schema: a TicDatFactory or PanDatFactory defining the output schema :param solve: a function that takes a input_schema.TicDat object and returns a solution_schema.TicDat object :param case_space_table_names - passed through to any TicDatFactory/PanDatFactory write functions that have case_space_table_names as an argument. Will also pass through to case_space_sheet_names for Excel writers. boolean - make best guesses how to add spaces and upper case characters to table names when writing to the file system. :return: None Implements a command line signature of \"python engine_file.py --input --output --roundoff \" For the input/output command line arguments. --> endings in \".xls\" or \".xlsx\" imply reading/writing Excel files --> endings in \".mdb\" or \".accdb\" imply reading/writing Access files (TicDatFactory only) --> ending in \".db\" imply reading/writing SQLite database files --> ending in \".sql\" imply reading/writing SQLite text files rendered in schema-less SQL statements (TicDatFactory only) --> ending in \".json\" imply reading/writing .json files --> otherwise, the assumption is that an input/output directory is being specified, which will be used for reading/writing .csv files. (Recall that .csv format is implemented as one-csv-file-per-table, so an entire model will be stored in a directory containing a series of .csv files) Defaults are input.xlsx, output.xlsx The roundoff config file is optional. See ticdat wiki for a description of the roundoff config file. verify verify(b, msg) raise a TicDatError exception if the boolean condition is False :param b: boolean condition. :param msg: string argument to the TicDatError construction :return: gurobi_env gurobi_env(*args, **kwargs) Return an object that can be passed to gurobipy.Model() as the env argument. On an ordinary Python installation, just returns None Useful for Gurobi licensing/DRM issues. :return: An object that can be passed to gurobipy.Model as the env argument ampl_format ampl_format(mod_str, **kwargs) Return a formatted version of mod_str, using substitutions from kwargs. The substitutions are identified by doubled-braces ('{{' and '}}'). Very similar to str.format, except single braces are left unmolested and double-braces are used to identify substitutions. This allows AMPL mod code to be more readable to AMPL developers. :param mod_str: the string that has doubled-braced substitutions entries. :param kwargs: Named arguments map from substitution-entry label to value. :return: A copy of mod_str with the substitutions performed. Slicer Slicer(self, iter_of_iters) Object to perform multi-index slicing over an index sequence slice Slicer.slice(self, *args) Perform a multi-index slice. (Not to be confused with the native Python slice) :param args a series of index values or ' '. The latter means 'match every value' :return: a list of tuples which match args. :caveat will run faster if gurobipy is available and tuplelist can accommodate the interior iterables clear Slicer.clear(self) reduce memory overheard by clearing out any archived slicing. this is a no-op if gurobipy is available :return: Sloc Sloc(self, s) A utility class for the slicing on pandas Series. Works just like .loc, except doesn't exception out when encountering an empty slice. All credit for this class goes to the inimitable IL. https://github.com/pydata/pandas/issues/10695 add_sloc Sloc.add_sloc(s) adds an .sloc attribute to a the series or to every column of the data frame :param s: either a series or a data frame :return: s if .sloc could be added, None otherwise LogFile LogFile(self, path) Utility class for writing log files. Also enables writing on-the-fly tables into log files. log_table LogFile.log_table(self, table_name, seq, formatter=<function LogFile.<lambda> at 0x118f9b1e0>, max_write=10) Writes a table to the log file. Extremely useful functionality for on the fly errors, warnings and diagnostics. :param log_table : the name to be given to the logged table :param seq: An iterable of iterables. The first iterable lists the field names for the table. The remaining iterables list the column values for each row. The outer iterable is thus of length num_rows + 1, while each of the inner iterables are of length num_cols. :param formatter: a function used to turn column entries into strings :param max_write: the maximum number of table entries to write to the actual log file. :return: Progress Progress(self, quiet=False) Utility class for indicating progress. numerical_progress Progress.numerical_progress(self, theme, progress) indicate generic progress :param theme: string describing the type of progress being advanced :param progress: numerical indicator to the degree of progress advanced :return: False if GUI indicates solve should gracefully finish, True otherwise mip_progress Progress.mip_progress(self, theme, lower_bound, upper_bound) indicate progress towards solving a MIP via converging upper and lower bounds :param theme: string describing the type of MIP solve underway :param lower_bound: the best current lower bound to the MIP objective :param upper_bound: the best current upper bound to the MIP objective :return: False if GUI indicates solve should gracefully finish, True otherwise gurobi_call_back_factory Progress.gurobi_call_back_factory(self, theme, model) Allow a Gurobi model to call mip_progress. Only for minimize :param theme: string describing the type of MIP solve underway :param model: a Gurobi model (or ticdat.Model.core_model) :return: a call_back function that can be passed to Model.optimize add_cplex_listener Progress.add_cplex_listener(self, theme, model) Allow a CPLEX model to call mip_progress. Only for minimize :param theme: short descriptive string :param model: cplex.Model object (or ticdat.Model.core_model) :return:","title":"Utilities"},{"location":"xls/","text":"XlsTicFactory XlsTicFactory(self, tic_dat_factory) Primary class for reading/writing Excel files with TicDat objects. Your system will need the xlrd package to read .xls files, the openpyxl package to read xlsx/xlsm/xltx/xltm files, the xlwt package to write .xls files, and the xlsxwriter package to write .xlsx files. Don't create this object explicitly. A XlsTicDatFactory will automatically be associated with the xls attribute of the parent TicDatFactory. create_tic_dat XlsTicFactory.create_tic_dat(self, xls_file_path, row_offsets=None, headers_present=True, treat_inf_as_infinity=True, freeze_it=False) Create a TicDat object from an Excel file :param xls_file_path: An Excel file containing sheets whose names match the table names in the schema. :param row_offsets: (optional) A mapping from table names to initial number of rows to skip :param headers_present: Boolean. Does the first row of data contain the column headers? :param treat_inf_as_infinity: Boolean. Treat the \"inf\" string (case insensitive) as as infinity. Similar for \"-inf\" :param freeze_it: boolean. should the returned object be frozen? :return: a TicDat object populated by the matching sheets. caveats: Missing sheets resolve to an empty table, but missing fields on matching sheets throw an Exception. Sheet names are considered case insensitive, and white space is replaced with underscore for table name matching. Field names are considered case insensitive, but white space is respected. (ticdat supports whitespace in field names but not table names). The following two caveats apply only if data_types are used. --> Any field for which an empty string is invalid data and None is valid data will replace the empty string with None. --> Any field for which must_be_int is true will replace numeric data that satisfies int(x)==x with int(x). In other words, the ticdat equivalent of pandas.read_excel convert_float is to set must_be_int to true in data_types. find_duplicates XlsTicFactory.find_duplicates(self, xls_file_path, row_offsets={}, headers_present=True) Find the row counts for duplicated rows. :param xls_file_path: An Excel file containing sheets whose names match the table names in the schema (non primary key tables ignored). :param row_offsets: (optional) A mapping from table names to initial number of rows to skip (non primary key tables ignored) :param headers_present: Boolean. Does the first row of data contain the column headers? caveats: Missing sheets resolve to an empty table, but missing primary fields on matching sheets throw an Exception. Sheet names are considered case insensitive. :return: A dictionary whose keys are the table names for the primary key tables. Each value of the return dictionary is itself a dictionary. The inner dictionary is keyed by the primary key values encountered in the table, and the value is the count of records in the Excel sheet with this primary key. Row counts smaller than 2 are pruned off, as they aren't duplicates write_file XlsTicFactory.write_file(self, tic_dat, file_path, allow_overwrite=False, case_space_sheet_names=False) write the ticDat data to an excel file :param tic_dat: the data object to write (typically a TicDat) :param file_path: The file path of the excel file to create Needs to end in either \".xls\" or \".xlsx\" The latter is capable of writing out larger tables, but the former handles infinity seamlessly. If \".xlsx\" then be advised that +/- float(\"inf\") will be replaced with \"inf\"/\"-inf\", unless infinity_io_flag is being applied. :param allow_overwrite: boolean - are we allowed to overwrite an existing file? case_space_sheet_names: boolean - make best guesses how to add spaces and upper case characters to sheet names :return: caveats: None may be written out as an empty string. This reflects the behavior of xlwt.","title":"XlsTicFactory"}]}